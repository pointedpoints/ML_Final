{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u5ZZose_WE4g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import zipfile\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Q-Learning parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount rate\n",
        "epsilon = 0.1  # Epsilon-greedy strategy probability\n",
        "episodes = 1000  # Number of episodes\n",
        "\n",
        "# Extract files from the ZIP archive\n",
        "def extract_files(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "zip_path = '/content/Supplement materials(1) (1).zip'  # the path to ZIP file\n",
        "extract_files(zip_path)\n",
        "\n",
        "# Load the map and coordinates to create the grid\n",
        "def load_map_and_coords(map_path, coords_path):\n",
        "    map_image = Image.open(map_path)\n",
        "    coords_image = Image.open(coords_path)\n",
        "\n",
        "    map_grid = np.array(map_image)\n",
        "    coords_grid = np.array(coords_image)\n",
        "\n",
        "    return map_grid, coords_grid\n",
        "\n",
        "map_path = 'Tsinghua map net grid.jpg'  # the path to map file\n",
        "coords_path = 'Tsinghua map net coords.jpg'  # the path to coords file\n",
        "map_grid, coords_grid = load_map_and_coords(map_path, coords_path)"
      ],
      "metadata": {
        "id": "newZuaSmPACP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i define the states, actions, and rewards\n",
        "states = [(x, y) for x in range(map_grid.shape[0]) for y in range(map_grid.shape[1])]\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "points_of_interest = []  # List of points of interest\n",
        "entrance_gates = []  # List of entrance gates\n",
        "exit_gates = []  # List of exit gates\n",
        "\n",
        "# Initialize Q-table\n",
        "Q_table = np.zeros((len(states), len(actions)))\n"
      ],
      "metadata": {
        "id": "aBB_0nN-PU-1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i define the reward function\n",
        "def get_reward(state, action, is_car):\n",
        "    x, y = state\n",
        "    if state in points_of_interest:\n",
        "        return 10\n",
        "    elif state in exit_gates:\n",
        "        return 20\n",
        "    elif is_car and (x, y) in forbidden_zones:  # forbidden_zones is a list of forbidden zones for cars\n",
        "        return -10\n",
        "    else:\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "Nsvv350kPboE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i defined the possible actions function\n",
        "def get_possible_actions(state, is_car):\n",
        "    x, y = state\n",
        "    possible_actions = []\n",
        "    if x > 0 and (x-1, y) in states and not is_car or not (x-1, y) in forbidden_zones:\n",
        "        possible_actions.append('up')\n",
        "    if x < map_grid.shape[0] - 1 and (x+1, y) in states and not is_car or not (x+1, y) in forbidden_zones:\n",
        "        possible_actions.append('down')\n",
        "    if y > 0 and (x, y-1) in states and not is_car or not (x, y-1) in forbidden_zones:\n",
        "        possible_actions.append('left')\n",
        "    if y < map_grid.shape[1] - 1 and (x, y+1) in states and not is_car or not (x, y+1) in forbidden_zones:\n",
        "        possible_actions.append('right')\n",
        "    return possible_actions\n"
      ],
      "metadata": {
        "id": "2PqVIIKfPgYt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i defined the step function\n",
        "def step(state, action):\n",
        "    x, y = state\n",
        "    if action == 'up':\n",
        "        next_state = (x-1, y)\n",
        "    elif action == 'down':\n",
        "        next_state = (x+1, y)\n",
        "    elif action == 'left':\n",
        "        next_state = (x, y-1)\n",
        "    elif action == 'right':\n",
        "        next_state = (x, y+1)\n",
        "\n",
        "    is_car = random.choice([True, False])  # i randomly choose the type of visitor\n",
        "    reward = get_reward(next_state, action, is_car)\n",
        "    done = next_state in exit_gates\n",
        "    return next_state, reward, done\n"
      ],
      "metadata": {
        "id": "kf9OBb5NPvau"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the agent\n",
        "for episode in range(episodes):\n",
        "    if not entrance_gates:  # need check if the list is empty\n",
        "        print(\"No entrance gates defined. Please define entrance gates.\")\n",
        "        break\n",
        "    state = random.choice(entrance_gates)  # i started from a random entrance gate\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.random.choice(get_possible_actions(state, random.choice([True, False])))\n",
        "        next_state, reward, done = step(state, action)\n",
        "        old_value = Q_table[states.index(state), actions.index(action)]\n",
        "        next_max = np.max(Q_table[states.index(next_state)])\n",
        "\n",
        "        # Update Q-table\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        Q_table[states.index(state), actions.index(action)] = new_value\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "# After training, our Q-table will contain the optimal values for each state and action\n",
        "print(\"Q-table after training:\")\n",
        "print(Q_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71tQrEh5P0oB",
        "outputId": "d3aee151-d863-4351-c918-f459be59c494"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No entrance gates defined. Please define entrance gates.\n",
            "Q-table after training:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i defined the Q-Learning parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount rate\n",
        "epsilon = 0.1  # Epsilon-greedy strategy probability\n",
        "episodes = 1000  # Number of episodes\n",
        "\n",
        "# i extracted files from the ZIP archive\n",
        "def extract_files(zip_path):\n",
        "    with zipfile.ZipFile('/content/Supplement materials(1) (1).zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "zip_path = 'Supplement materials.zip'  #  the path to ZIP file\n",
        "extract_files(zip_path)\n",
        "\n",
        "# i loaded the map and coordinates to create the grid\n",
        "def load_map_and_coords(map_path, coords_path):\n",
        "    map_image = Image.open(map_path)\n",
        "    coords_image = Image.open(coords_path)\n",
        "\n",
        "    # Converted images to grid\n",
        "    map_grid = np.array(map_image)\n",
        "    coords_grid = np.array(coords_image)\n",
        "\n",
        "    return map_grid, coords_grid\n",
        "\n",
        "map_path = 'Tsinghua map net grid.jpg'\n",
        "coords_path = 'Tsinghua map net coords.jpg'\n",
        "map_grid, coords_grid = load_map_and_coords(map_path, coords_path)\n",
        "\n",
        "# Defined the states, actions, and rewards\n",
        "states = [(x, y) for x in range(map_grid.shape[0]) for y in range(map_grid.shape[1])]\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "points_of_interest = [(2, 3), (5, 7), (1, 5), (4, 6), (3, 2)]  # points of interest\n",
        "entrance_gates = [(0, 0), (9, 0), (0, 9), (9, 9)]  # entrance gates\n",
        "exit_gates = [(8, 8)]  # exit gates\n",
        "\n",
        "# Initialize Q-table\n",
        "Q_table = np.zeros((len(states), len(actions)))\n"
      ],
      "metadata": {
        "id": "9HwYK9_4Qcfd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i defined the reward function\n",
        "def get_reward(state, action, is_car):\n",
        "    x, y = state\n",
        "    if state in points_of_interest:\n",
        "        return 10\n",
        "    elif state in exit_gates:\n",
        "        return 20\n",
        "    elif is_car and (x, y) in [(4, 4), (5, 5)]:  # forbidden zones for cars\n",
        "        return -10\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# i defined the possible actions function\n",
        "def get_possible_actions(state, is_car):\n",
        "    x, y = state\n",
        "    possible_actions = []\n",
        "    if x > 0 and (x-1, y) in states and not is_car or not (x-1, y) in [(4, 4), (5, 5)]:\n",
        "        possible_actions.append('up')\n",
        "    if x < map_grid.shape[0] - 1 and (x+1, y) in states and not is_car or not (x+1, y) in [(4, 4), (5, 5)]:\n",
        "        possible_actions.append('down')\n",
        "    if y > 0 and (x, y-1) in states and not is_car or not (x, y-1) in [(4, 4), (5, 5)]:\n",
        "        possible_actions.append('left')\n",
        "    if y < map_grid.shape[1] - 1 and (x, y+1) in states and not is_car or not (x, y+1) in [(4, 4), (5, 5)]:\n",
        "        possible_actions.append('right')\n",
        "    return possible_actions"
      ],
      "metadata": {
        "id": "ijw8NsvgR1wm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i defined the step function\n",
        "def step(state, action):\n",
        "    x, y = state\n",
        "    if action == 'up':\n",
        "        next_state = (x - 1, y)\n",
        "    elif action == 'down':\n",
        "        next_state = (x + 1, y)\n",
        "    elif action == 'left':\n",
        "        next_state = (x, y - 1)\n",
        "    elif action == 'right':\n",
        "        next_state = (x, y + 1)\n",
        "\n",
        "\n",
        "    next_state = (max(0, min(map_grid.shape[0] - 1, next_state[0])),\n",
        "                  max(0, min(map_grid.shape[1] - 1, next_state[1])))\n",
        "\n",
        "    # i determined reward and if the episode is done\n",
        "    if next_state in exit_gates:\n",
        "        done = True\n",
        "        reward = 20  # Reward for reaching the exit gate\n",
        "    elif next_state in points_of_interest:\n",
        "        done = False\n",
        "        reward = 10  # Reward for visiting a point of interest\n",
        "    elif (next_state[0], next_state[1]) in [(4, 4), (5, 5)]:  # Forbidden zones for cars\n",
        "        done = False\n",
        "        reward = -10\n",
        "    else:\n",
        "        done = False\n",
        "        reward = 0\n",
        "\n",
        "    return next_state, reward, done\n"
      ],
      "metadata": {
        "id": "vI1hIMnUSD8s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# im training the agent\n",
        "for episode in range(episodes):\n",
        "    state = random.choice(entrance_gates)  # Start from a random entrance gate\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Get possible actions for the current state\n",
        "        possible_actions = get_possible_actions(state)\n",
        "\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(possible_actions)\n",
        "        else:\n",
        "            # Select the action with the highest Q-value\n",
        "            action = actions[np.argmax([Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),\n",
        "                                         actions.index(a)] for a in possible_actions])]\n",
        "\n",
        "        next_state, reward, done = step(state, action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "pTksTPm9Zafn",
        "outputId": "87a32e3c-ec38-4a1f-e5d0-34444963844f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "get_possible_actions() missing 1 required positional argument: 'is_car'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-185c31213594>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Get possible actions for the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpossible_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_possible_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Choose action using epsilon-greedy policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_possible_actions() missing 1 required positional argument: 'is_car'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        # Update Q-table using Q-learning update rule\n",
        "        old_value = Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),\n",
        "                            actions.index(action)]\n",
        "        next_max = np.max(Q_table[state_to_index(next_state, map_grid.shape[0], map_grid.shape[1])])\n",
        "\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),\n",
        "                actions.index(action)] = new_value\n",
        "\n",
        "        state = next_state  # Move to the next state"
      ],
      "metadata": {
        "id": "rcKGHAIAZe8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "ffd576a7-ad0e-41e5-d145-b33ad2a2e150"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'state_to_index' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-85a1d574d554>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Update Q-table using Q-learning update rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m old_value = Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),\n\u001b[0m\u001b[1;32m      3\u001b[0m                     actions.index(action)]\n\u001b[1;32m      4\u001b[0m \u001b[0mnext_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state_to_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Q-Learning Parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount rate\n",
        "epsilon = 0.1  # Epsilon-greedy strategy probability\n",
        "episodes = 1000  # Number of episodes"
      ],
      "metadata": {
        "id": "o1E0Mcnh7pW0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract files from the archive\n",
        "def extract_files(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "zip_path = '/content/Supplement materials(1) (1).zip'\n",
        "extract_files(zip_path)\n",
        "\n",
        "# Function to load map and coordinates\n",
        "def load_map_and_coords(map_path, coords_path):\n",
        "    map_image = Image.open(map_path)\n",
        "    coords_image = Image.open(coords_path)\n",
        "    map_grid = np.array(map_image)\n",
        "    coords_grid = np.array(coords_image)\n",
        "    return map_grid, coords_grid\n",
        "\n",
        "map_path = 'Tsinghua map net grid.jpg'  # Map file path\n",
        "coords_path = 'Tsinghua map net coords.jpg'  # Path to the coordinate file\n",
        "map_grid, coords_grid = load_map_and_coords(map_path, coords_path)"
      ],
      "metadata": {
        "id": "IQemfO8Z8HVE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i defined states, actions, and rewards\n",
        "states = [(x, y) for x in range(map_grid.shape[0]) for y in range(map_grid.shape[1])]\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "points_of_interest = [(2, 3), (5, 7), (1, 5), (4, 6), (3, 2)]\n",
        "entrance_gates = [(0, 0), (9, 0), (0, 9), (9, 9)]\n",
        "exit_gates = [(8, 8)]\n",
        "\n",
        "# Function to convert state to index\n",
        "def state_to_index(state, width, height):\n",
        "    x, y = state\n",
        "    return x * width + y\n",
        "\n",
        "# Function for determining remuneration\n",
        "def get_reward(state, action, is_car):\n",
        "    x, y = state\n",
        "    if state in points_of_interest:\n",
        "        return 10\n",
        "    elif state in exit_gates:\n",
        "        return 20\n",
        "    elif is_car and (x, y) in [(4, 4), (5, 5)]:  # No Car Zones\n",
        "        return -10\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to define possible actions\n",
        "def get_possible_actions(state, is_car):\n",
        "    x, y = state\n",
        "    possible_actions = []\n",
        "    if x > 0 and (x-1, y) in states and not is_car or not (x-1, y) in [(4, 4), (5, 5)]:\n",
        "        possible_actions.append('up')\n",
        "    if x < map_grid.shape[0] - 1 and (x+1, y) in states and not is_car or not (x+1, y) in [(4, 4), (5, 5)]:\n",
        "        possible_actions.append('down')\n",
        "    if y > 0 and (x, y-1) in states and not is_car or not (x, y-1) in [(4, 4), (5, 5)]:\n",
        "        possible_actions.append('left')\n",
        "    if y < map_grid.shape[1] - 1 and (x, y+1) in states and not is_car or not (x, y+1) in [(4, 4), (5, 5)]:\n",
        "        possible_actions.append('right')\n",
        "    return possible_actions\n",
        "\n",
        "# Function to determine the next state\n",
        "def step(state, action):\n",
        "    x, y = state\n",
        "    if action == 'up':\n",
        "        next_state = (x - 1, y)\n",
        "    elif action == 'down':\n",
        "        next_state = (x + 1, y)\n",
        "    elif action == 'left':\n",
        "        next_state = (x, y - 1)\n",
        "    elif action == 'right':\n",
        "        next_state = (x, y + 1)\n",
        "\n",
        "\n",
        "    next_state = (max(0, min(map_grid.shape[0] - 1, next_state[0])),\n",
        "                     max(0, min(map_grid.shape[1] - 1, next_state[1])))\n",
        "\n",
        "    # Determined reward and look if the episode is done\n",
        "    if next_state in exit_gates:\n",
        "        done = True\n",
        "        reward = 20  # Reward for reaching the exit gate\n",
        "    elif next_state in points_of_interest:\n",
        "        done = False\n",
        "        reward = 10  # Reward for visiting a point of interest\n",
        "    elif (next_state[0], next_state[1]) in [(4, 4), (5, 5)]:  # Forbidden zones for cars\n",
        "        done = False\n",
        "        reward = -10\n",
        "    else:\n",
        "        done = False\n",
        "        reward = 0\n",
        "\n",
        "    return next_state, reward, done\n"
      ],
      "metadata": {
        "id": "45ZKFdkE8TPl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Q-table\n",
        "Q_table = np.zeros((len(states), len(actions)))"
      ],
      "metadata": {
        "id": "b4faHe_88zkf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent Training\n",
        "for episode in range(episodes):\n",
        "    state = random.choice(entrance_gates)  # Start from a random entrance gate\n",
        "    is_car = True  # Set to True if the agent is a car, False otherwise\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Get possible actions for the current state\n",
        "        possible_actions = get_possible_actions(state, is_car)\n",
        "\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(possible_actions)\n",
        "        else:\n",
        "            # Select the action with the highest Q-value\n",
        "            action = actions[np.argmax([Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),\n",
        "                                              actions.index(a)] for a in possible_actions])]\n",
        "\n",
        "        next_state, reward, done = step(state, action)\n",
        "        # Update Q-table\n",
        "        old_value = Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]), actions.index(action)]\n",
        "        next_max = np.max(Q_table[state_to_index(next_state, map_grid.shape[0], map_grid.shape[1])])\n",
        "        Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]), actions.index(action)] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "        state = next_state\n"
      ],
      "metadata": {
        "id": "9M-BNMWR83tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize the current state of the agent on the map\n",
        "def visualize_agent_state(map_grid, state, points_of_interest, entrance_gates, exit_gates):\n",
        "    # Create a copy of the map for display\n",
        "    map_visual = map_grid.copy()\n",
        "\n",
        "    # Mark points of interest, entrance and exit gates\n",
        "    for point in points_of_interest:\n",
        "        map_visual[point] = [0, 255, 0]  # Green color\n",
        "    for gate in entrance_gates:\n",
        "        map_visual[gate] = [0, 0, 255]  # Blue color\n",
        "    for gate in exit_gates:\n",
        "        map_visual[gate] = [255, 0, 0]  # Red color\n",
        "\n",
        "    # Marking the current state of the agent\n",
        "    map_visual[state] = [255, 255, 0]  # Yellow\n",
        "\n",
        "    # Display the map\n",
        "    plt.imshow(map_visual)\n",
        "    plt.title(\"Agent State Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "# Q-table visualization function\n",
        "def visualize_q_table(Q_table, states, actions):\n",
        "    # Create a heatmap to display Q-values\n",
        "    q_values = np.max(Q_table, axis=1).reshape(map_grid.shape[0], map_grid.shape[1])\n",
        "    plt.imshow(q_values, cmap='hot', interpolation='nearest')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Q-Table Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "# Adding visualization to the learning process\n",
        "for episode in range(episodes):\n",
        "    state = random.choice(entrance_gates)\n",
        "    is_car = True\n",
        "    done = False\n",
        "    while not done:\n",
        "        possible_actions = get_possible_actions(state, is_car)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(possible_actions)\n",
        "        else:\n",
        "            action = actions[np.argmax([Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),\n",
        "                                          actions.index(a)] for a in possible_actions])]\n",
        "\n",
        "        next_state, reward, done = step(state, action)\n",
        "\n",
        "        # Q-Table Update\n",
        "        old_value = Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]), actions.index(action)]\n",
        "        next_max = np.max(Q_table[state_to_index(next_state, map_grid.shape[0], map_grid.shape[1])])\n",
        "        Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]), actions.index(action)] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "        state = next_state\n",
        "\n",
        "        # Visualization of the current state of the agent on the map\n",
        "        if episode % 100 == 0:  # We visualize every 100 episodes\n",
        "            visualize_agent_state(map_grid, state, points_of_interest, entrance_gates, exit_gates)\n",
        "\n",
        "# Q-table visualization after training\n",
        "visualize_q_table(Q_table, states, actions)\n"
      ],
      "metadata": {
        "id": "72JYlTx0-Lns"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}