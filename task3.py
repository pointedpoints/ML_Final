# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.
"""

import numpy as np
import random
import zipfile
from PIL import Image

# Define the Q-Learning parameters
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount rate
epsilon = 0.1  # Epsilon-greedy strategy probability
episodes = 1000  # Number of episodes

# Extract files from the ZIP archive
def extract_files(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall()

zip_path = '/content/Supplement materials(1) (1).zip'  # the path to ZIP file
extract_files(zip_path)

# Load the map and coordinates to create the grid
def load_map_and_coords(map_path, coords_path):
    map_image = Image.open(map_path)
    coords_image = Image.open(coords_path)

    map_grid = np.array(map_image)
    coords_grid = np.array(coords_image)

    return map_grid, coords_grid

map_path = 'Tsinghua map net grid.jpg'  # the path to map file
coords_path = 'Tsinghua map net coords.jpg'  # the path to coords file
map_grid, coords_grid = load_map_and_coords(map_path, coords_path)

# i define the states, actions, and rewards
states = [(x, y) for x in range(map_grid.shape[0]) for y in range(map_grid.shape[1])]
actions = ['up', 'down', 'left', 'right']
points_of_interest = []  # List of points of interest
entrance_gates = []  # List of entrance gates
exit_gates = []  # List of exit gates

# Initialize Q-table
Q_table = np.zeros((len(states), len(actions)))

# i define the reward function
def get_reward(state, action, is_car):
    x, y = state
    if state in points_of_interest:
        return 10
    elif state in exit_gates:
        return 20
    elif is_car and (x, y) in forbidden_zones:  # forbidden_zones is a list of forbidden zones for cars
        return -10
    else:
        return 0

# i defined the possible actions function
def get_possible_actions(state, is_car):
    x, y = state
    possible_actions = []
    if x > 0 and (x-1, y) in states and not is_car or not (x-1, y) in forbidden_zones:
        possible_actions.append('up')
    if x < map_grid.shape[0] - 1 and (x+1, y) in states and not is_car or not (x+1, y) in forbidden_zones:
        possible_actions.append('down')
    if y > 0 and (x, y-1) in states and not is_car or not (x, y-1) in forbidden_zones:
        possible_actions.append('left')
    if y < map_grid.shape[1] - 1 and (x, y+1) in states and not is_car or not (x, y+1) in forbidden_zones:
        possible_actions.append('right')
    return possible_actions

# i defined the step function
def step(state, action):
    x, y = state
    if action == 'up':
        next_state = (x-1, y)
    elif action == 'down':
        next_state = (x+1, y)
    elif action == 'left':
        next_state = (x, y-1)
    elif action == 'right':
        next_state = (x, y+1)

    is_car = random.choice([True, False])  # i randomly choose the type of visitor
    reward = get_reward(next_state, action, is_car)
    done = next_state in exit_gates
    return next_state, reward, done

# Training the agent
for episode in range(episodes):
    if not entrance_gates:  # need check if the list is empty
        print("No entrance gates defined. Please define entrance gates.")
        break
    state = random.choice(entrance_gates)  # i started from a random entrance gate
    done = False
    while not done:
        action = np.random.choice(get_possible_actions(state, random.choice([True, False])))
        next_state, reward, done = step(state, action)
        old_value = Q_table[states.index(state), actions.index(action)]
        next_max = np.max(Q_table[states.index(next_state)])

        # Update Q-table
        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
        Q_table[states.index(state), actions.index(action)] = new_value

        state = next_state

# After training, our Q-table will contain the optimal values for each state and action
print("Q-table after training:")
print(Q_table)

# i defined the Q-Learning parameters
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount rate
epsilon = 0.1  # Epsilon-greedy strategy probability
episodes = 1000  # Number of episodes

# i extracted files from the ZIP archive
def extract_files(zip_path):
    with zipfile.ZipFile('/content/Supplement materials(1) (1).zip', 'r') as zip_ref:
        zip_ref.extractall()

zip_path = 'Supplement materials.zip'  #  the path to ZIP file
extract_files(zip_path)

# i loaded the map and coordinates to create the grid
def load_map_and_coords(map_path, coords_path):
    map_image = Image.open(map_path)
    coords_image = Image.open(coords_path)

    # Converted images to grid
    map_grid = np.array(map_image)
    coords_grid = np.array(coords_image)

    return map_grid, coords_grid

map_path = 'Tsinghua map net grid.jpg'
coords_path = 'Tsinghua map net coords.jpg'
map_grid, coords_grid = load_map_and_coords(map_path, coords_path)

# Defined the states, actions, and rewards
states = [(x, y) for x in range(map_grid.shape[0]) for y in range(map_grid.shape[1])]
actions = ['up', 'down', 'left', 'right']
points_of_interest = [(2, 3), (5, 7), (1, 5), (4, 6), (3, 2)]  # points of interest
entrance_gates = [(0, 0), (9, 0), (0, 9), (9, 9)]  # entrance gates
exit_gates = [(8, 8)]  # exit gates

# Initialize Q-table
Q_table = np.zeros((len(states), len(actions)))

# i defined the reward function
def get_reward(state, action, is_car):
    x, y = state
    if state in points_of_interest:
        return 10
    elif state in exit_gates:
        return 20
    elif is_car and (x, y) in [(4, 4), (5, 5)]:  # forbidden zones for cars
        return -10
    else:
        return 0

# i defined the possible actions function
def get_possible_actions(state, is_car):
    x, y = state
    possible_actions = []
    if x > 0 and (x-1, y) in states and not is_car or not (x-1, y) in [(4, 4), (5, 5)]:
        possible_actions.append('up')
    if x < map_grid.shape[0] - 1 and (x+1, y) in states and not is_car or not (x+1, y) in [(4, 4), (5, 5)]:
        possible_actions.append('down')
    if y > 0 and (x, y-1) in states and not is_car or not (x, y-1) in [(4, 4), (5, 5)]:
        possible_actions.append('left')
    if y < map_grid.shape[1] - 1 and (x, y+1) in states and not is_car or not (x, y+1) in [(4, 4), (5, 5)]:
        possible_actions.append('right')
    return possible_actions

# i defined the step function
def step(state, action):
    x, y = state
    if action == 'up':
        next_state = (x - 1, y)
    elif action == 'down':
        next_state = (x + 1, y)
    elif action == 'left':
        next_state = (x, y - 1)
    elif action == 'right':
        next_state = (x, y + 1)


    next_state = (max(0, min(map_grid.shape[0] - 1, next_state[0])),
                  max(0, min(map_grid.shape[1] - 1, next_state[1])))

    # i determined reward and if the episode is done
    if next_state in exit_gates:
        done = True
        reward = 20  # Reward for reaching the exit gate
    elif next_state in points_of_interest:
        done = False
        reward = 10  # Reward for visiting a point of interest
    elif (next_state[0], next_state[1]) in [(4, 4), (5, 5)]:  # Forbidden zones for cars
        done = False
        reward = -10
    else:
        done = False
        reward = 0

    return next_state, reward, done

# im training the agent
for episode in range(episodes):
    state = random.choice(entrance_gates)  # Start from a random entrance gate
    done = False
    while not done:
        # Get possible actions for the current state
        possible_actions = get_possible_actions(state)

        # Choose action using epsilon-greedy policy
        if random.uniform(0, 1) < epsilon:
            action = random.choice(possible_actions)
        else:
            # Select the action with the highest Q-value
            action = actions[np.argmax([Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),
                                         actions.index(a)] for a in possible_actions])]

        next_state, reward, done = step(state, action)

# Update Q-table using Q-learning update rule
        old_value = Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),
                            actions.index(action)]
        next_max = np.max(Q_table[state_to_index(next_state, map_grid.shape[0], map_grid.shape[1])])

        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
        Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),
                actions.index(action)] = new_value

        state = next_state  # Move to the next state

# Defining Q-Learning Parameters
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount rate
epsilon = 0.1  # Epsilon-greedy strategy probability
episodes = 1000  # Number of episodes

# Function to extract files from the archive
def extract_files(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall()

zip_path = '/content/Supplement materials(1) (1).zip'
extract_files(zip_path)

# Function to load map and coordinates
def load_map_and_coords(map_path, coords_path):
    map_image = Image.open(map_path)
    coords_image = Image.open(coords_path)
    map_grid = np.array(map_image)
    coords_grid = np.array(coords_image)
    return map_grid, coords_grid

map_path = 'Tsinghua map net grid.jpg'  # Map file path
coords_path = 'Tsinghua map net coords.jpg'  # Path to the coordinate file
map_grid, coords_grid = load_map_and_coords(map_path, coords_path)

# i defined states, actions, and rewards
states = [(x, y) for x in range(map_grid.shape[0]) for y in range(map_grid.shape[1])]
actions = ['up', 'down', 'left', 'right']
points_of_interest = [(2, 3), (5, 7), (1, 5), (4, 6), (3, 2)]
entrance_gates = [(0, 0), (9, 0), (0, 9), (9, 9)]
exit_gates = [(8, 8)]

# Function to convert state to index
def state_to_index(state, width, height):
    x, y = state
    return x * width + y

# Function for determining remuneration
def get_reward(state, action, is_car):
    x, y = state
    if state in points_of_interest:
        return 10
    elif state in exit_gates:
        return 20
    elif is_car and (x, y) in [(4, 4), (5, 5)]:  # No Car Zones
        return -10
    else:
        return 0

# Function to define possible actions
def get_possible_actions(state, is_car):
    x, y = state
    possible_actions = []
    if x > 0 and (x-1, y) in states and not is_car or not (x-1, y) in [(4, 4), (5, 5)]:
        possible_actions.append('up')
    if x < map_grid.shape[0] - 1 and (x+1, y) in states and not is_car or not (x+1, y) in [(4, 4), (5, 5)]:
        possible_actions.append('down')
    if y > 0 and (x, y-1) in states and not is_car or not (x, y-1) in [(4, 4), (5, 5)]:
        possible_actions.append('left')
    if y < map_grid.shape[1] - 1 and (x, y+1) in states and not is_car or not (x, y+1) in [(4, 4), (5, 5)]:
        possible_actions.append('right')
    return possible_actions

# Function to determine the next state
def step(state, action):
    x, y = state
    if action == 'up':
        next_state = (x - 1, y)
    elif action == 'down':
        next_state = (x + 1, y)
    elif action == 'left':
        next_state = (x, y - 1)
    elif action == 'right':
        next_state = (x, y + 1)


    next_state = (max(0, min(map_grid.shape[0] - 1, next_state[0])),
                     max(0, min(map_grid.shape[1] - 1, next_state[1])))

    # Determined reward and look if the episode is done
    if next_state in exit_gates:
        done = True
        reward = 20  # Reward for reaching the exit gate
    elif next_state in points_of_interest:
        done = False
        reward = 10  # Reward for visiting a point of interest
    elif (next_state[0], next_state[1]) in [(4, 4), (5, 5)]:  # Forbidden zones for cars
        done = False
        reward = -10
    else:
        done = False
        reward = 0

    return next_state, reward, done

# Initializing the Q-table
Q_table = np.zeros((len(states), len(actions)))

# Agent Training
for episode in range(episodes):
    state = random.choice(entrance_gates)  # Start from a random entrance gate
    is_car = True  # Set to True if the agent is a car, False otherwise
    done = False
    while not done:
        # Get possible actions for the current state
        possible_actions = get_possible_actions(state, is_car)

        # Choose action using epsilon-greedy policy
        if random.uniform(0, 1) < epsilon:
            action = random.choice(possible_actions)
        else:
            # Select the action with the highest Q-value
            action = actions[np.argmax([Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),
                                              actions.index(a)] for a in possible_actions])]

        next_state, reward, done = step(state, action)
        # Update Q-table
        old_value = Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]), actions.index(action)]
        next_max = np.max(Q_table[state_to_index(next_state, map_grid.shape[0], map_grid.shape[1])])
        Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]), actions.index(action)] = old_value + alpha * (reward + gamma * next_max - old_value)
        state = next_state

# Function to visualize the current state of the agent on the map
def visualize_agent_state(map_grid, state, points_of_interest, entrance_gates, exit_gates):
    # Create a copy of the map for display
    map_visual = map_grid.copy()

    # Mark points of interest, entrance and exit gates
    for point in points_of_interest:
        map_visual[point] = [0, 255, 0]  # Green color
    for gate in entrance_gates:
        map_visual[gate] = [0, 0, 255]  # Blue color
    for gate in exit_gates:
        map_visual[gate] = [255, 0, 0]  # Red color

    # Marking the current state of the agent
    map_visual[state] = [255, 255, 0]  # Yellow

    # Display the map
    plt.imshow(map_visual)
    plt.title("Agent State Visualization")
    plt.show()

# Q-table visualization function
def visualize_q_table(Q_table, states, actions):
    # Create a heatmap to display Q-values
    q_values = np.max(Q_table, axis=1).reshape(map_grid.shape[0], map_grid.shape[1])
    plt.imshow(q_values, cmap='hot', interpolation='nearest')
    plt.colorbar()
    plt.title("Q-Table Visualization")
    plt.show()

# Adding visualization to the learning process
for episode in range(episodes):
    state = random.choice(entrance_gates)
    is_car = True
    done = False
    while not done:
        possible_actions = get_possible_actions(state, is_car)
        if random.uniform(0, 1) < epsilon:
            action = random.choice(possible_actions)
        else:
            action = actions[np.argmax([Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]),
                                          actions.index(a)] for a in possible_actions])]

        next_state, reward, done = step(state, action)

        # Q-Table Update
        old_value = Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]), actions.index(action)]
        next_max = np.max(Q_table[state_to_index(next_state, map_grid.shape[0], map_grid.shape[1])])
        Q_table[state_to_index(state, map_grid.shape[0], map_grid.shape[1]), actions.index(action)] = old_value + alpha * (reward + gamma * next_max - old_value)
        state = next_state

        # Visualization of the current state of the agent on the map
        if episode % 100 == 0:  # We visualize every 100 episodes
            visualize_agent_state(map_grid, state, points_of_interest, entrance_gates, exit_gates)

# Q-table visualization after training
visualize_q_table(Q_table, states, actions)