{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T07:43:49.356782Z",
     "start_time": "2025-01-03T07:43:49.353511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from QLearningAgent import QLearningAgent\n",
    "from PPORewardRecall import RewardCallback\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import stable_baselines3\n",
    "import gymnasium as gym\n",
    "\n",
    "from FrozenLakeRewardWrapper import FrozenLakeRewardWrapper\n",
    "from PPOAgent import PPOAgent\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 创建 FrozenLake 环境 (Q-Learning)\n",
    "map = ['SFHFFFFHFHFFFFFF', 'FFFHFFFFFFFFHFFF', 'FFFFHFFFFHFFHFHF', 'FFFFFFFFFFFFFFFF', 'FFHFFFFFFFFFFHFF', 'FFFHFFHFHFFFHFHF', 'HFFFHHFFHFFFFHHF', 'FFFFHFFFFFFFHFFF', 'FHFFFFFFFFFFFHFF', 'FHFFFFFFFFHFFFHH', 'FHFFFFFFFFHFHFFF', 'HFHFFHHFFFFFHFFF', 'FFFFFFFFFFFFHFFF', 'HFFFHFFHFFFFFFFF', 'FFFHFFFFFHFFFFFF', 'FFFHFFFFFFFHFFFG']#generate_random_map(size=16)"
   ],
   "id": "f2565e40334bc251",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-03T07:44:44.412931Z",
     "start_time": "2025-01-03T07:43:49.373643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_env1 = gym.make('FrozenLake-v1',desc=map, is_slippery = False)# map_name='4x4', is_slippery=True)#\n",
    "wrapped_training_env = FrozenLakeRewardWrapper(training_env1, step_penalty=0.02)\n",
    "\n",
    "eval_env = gym.make('FrozenLake-v1',desc=map, is_slippery = False)#map_name='4x4', is_slippery=True)#\n",
    "wrapped_eval_env = FrozenLakeRewardWrapper(eval_env, step_penalty=0.02)\n",
    "\n",
    "agent = QLearningAgent(wrapped_training_env)\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",            # 使用多层感知机策略\n",
    "    env=wrapped_training_env,      # 传入包装后的训练环境\n",
    "    verbose=0,                     # 显示训练过程的详细信息\n",
    "    learning_rate=5e-4,            # 设置学习率\n",
    "    clip_range=0.3,                # 设置剪切范围（epsilon_clip）\n",
    "    n_epochs=3,                    # 训练周期数\n",
    "    batch_size=128,                 # 批次大小\n",
    "    gamma=0.99,                    # 折扣因子\n",
    "    gae_lambda=0.95,               # GAE 的 lambda 参数\n",
    "    device='cpu',\n",
    "    policy_kwargs=dict(net_arch=[64, 64, 64]),\n",
    "    ent_coef= 0.02\n",
    ")\n",
    "reward_callback = RewardCallback(\n",
    "    eval_env=wrapped_eval_env,\n",
    "    eval_freq=5000,\n",
    "    n_eval_episodes=100,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "'''\n",
    "print(\"Start training...\")\n",
    "agent.train(episodes=300000)\n",
    "'''\n",
    "model.learn(\n",
    "    total_timesteps=100000,\n",
    "    callback=reward_callback\n",
    ")\n",
    "wrapped_training_env.close()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000\t平均奖励: -11.90 +/- 0.00\n",
      "Step 10000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 15000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 20000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 25000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 30000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 35000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 40000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 45000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 50000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 55000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 60000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 65000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 70000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 75000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 80000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 85000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 90000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 95000\t平均奖励: -2.10 +/- 0.00\n",
      "Step 100000\t平均奖励: -2.10 +/- 0.00\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T07:44:44.486818Z",
     "start_time": "2025-01-03T07:44:44.422887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "'''\n",
    "agent.env = wrapped_eval_env\n",
    "print(\"Start evaluation...\")\n",
    "agent.evaluate(episodes=100)\n",
    "'''\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    wrapped_eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True,  # 使用确定性策略进行评估\n",
    "    render=False\n",
    ")\n",
    "print(f\"Final Evaluation\\t平均奖励: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "id": "f5082eb8df7181c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation\t平均奖励: -2.10 +/- 0.00\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T07:45:34.745588Z",
     "start_time": "2025-01-03T07:45:33.325772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "final_env = gym.make('FrozenLake-v1', is_slippery=False, render_mode = 'human', desc = map)#, map_name='4x4')#\n",
    "wrapped_final_env = FrozenLakeRewardWrapper(final_env, step_penalty=0.02)\n",
    "\n",
    "#agent.env = wrapped_final_env\n",
    "model.set_env(wrapped_final_env)\n",
    "\n",
    "# 重置环境，获取初始状态\n",
    "state, info = wrapped_final_env.reset()\n",
    "\n",
    "done = False  # 游戏是否结束\n",
    "step_count = 0  # 计步\n",
    "\n",
    "while not done:\n",
    "    wrapped_final_env.render()  # 渲染环境\n",
    "    #action = agent.choose_best_action(state)  # 选择动作\n",
    "    #state, reward, terminated, truncated, _ = wrapped_final_env.step(action)  # 执行动作\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    state, reward, terminated, truncated, _ = wrapped_final_env.step(action[()])  # 执行动作\n",
    "    done = terminated or truncated\n",
    "    print(f\"Step {step_count}: Action={action}, State={state}, Reward={reward}, Done={done}\")\n",
    "    step_count += 1\n",
    "\n",
    "# 游戏结束后关闭环境\n",
    "wrapped_final_env.close()\n"
   ],
   "id": "d71e4ae8a9cafd2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Action=2, State=1, Reward=-0.1, Done=False\n",
      "Step 1: Action=2, State=2, Reward=-2.0, Done=True\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T07:44:46.434211Z",
     "start_time": "2025-01-03T07:44:46.429950Z"
    }
   },
   "cell_type": "code",
   "source": "print(map)",
   "id": "ea0a0d8750660e2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SFHFFFFHFHFFFFFF', 'FFFHFFFFFFFFHFFF', 'FFFFHFFFFHFFHFHF', 'FFFFFFFFFFFFFFFF', 'FFHFFFFFFFFFFHFF', 'FFFHFFHFHFFFHFHF', 'HFFFHHFFHFFFFHHF', 'FFFFHFFFFFFFHFFF', 'FHFFFFFFFFFFFHFF', 'FHFFFFFFFFHFFFHH', 'FHFFFFFFFFHFHFFF', 'HFHFFHHFFFFFHFFF', 'FFFFFFFFFFFFHFFF', 'HFFFHFFHFFFFFFFF', 'FFFHFFFFFHFFFFFF', 'FFFHFFFFFFFHFFFG']\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
