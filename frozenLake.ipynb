{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-30T09:18:44.640553Z",
     "start_time": "2024-12-30T09:14:33.009211Z"
    }
   },
   "source": [
    "from PPORewardRecall import RewardCallback\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import stable_baselines3\n",
    "import gymnasium as gym\n",
    "\n",
    "from FrozenLakeRewardWrapper import FrozenLakeRewardWrapper\n",
    "from PPOAgent import PPOAgent\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 创建 FrozenLake 环境 (Q-Learning)\n",
    "map = generate_random_map(size=16)\n",
    "training_env1 = gym.make('FrozenLake-v1', desc=map, is_slippery = False)#map_name='4x4', is_slippery=True)#\n",
    "wrapped_training_env = FrozenLakeRewardWrapper(training_env1)\n",
    "\n",
    "eval_env = gym.make('FrozenLake-v1', desc=map, is_slippery = False)#map_name='4x4', is_slippery=True）#\n",
    "wrapped_eval_env = FrozenLakeRewardWrapper(eval_env)\n",
    "\n",
    "# agent = QLearningAgent(wrapped_training_env)\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",            # 使用多层感知机策略\n",
    "    env=wrapped_training_env,      # 传入包装后的训练环境\n",
    "    verbose=0,                     # 显示训练过程的详细信息\n",
    "    learning_rate=5e-4,            # 设置学习率\n",
    "    clip_range=0.3,                # 设置剪切范围（epsilon_clip）\n",
    "    n_epochs=3,                    # 训练周期数\n",
    "    batch_size=128,                 # 批次大小\n",
    "    gamma=0.99,                    # 折扣因子\n",
    "    gae_lambda=0.95,               # GAE 的 lambda 参数\n",
    "    device='cpu',\n",
    "    policy_kwargs=dict(net_arch=[64, 64]),\n",
    "    ent_coef= 0.02\n",
    ")\n",
    "reward_callback = RewardCallback(\n",
    "    eval_env=wrapped_eval_env,\n",
    "    eval_freq=5000,\n",
    "    n_eval_episodes=100,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Start training...\")\n",
    "#agent.train(episodes=3000)\n",
    "model.learn(\n",
    "    total_timesteps=500000,\n",
    "    callback=reward_callback\n",
    ")\n",
    "wrapped_training_env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\final\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000\t平均奖励: -19.30 +/- 0.00\n",
      "Step 10000\t平均奖励: -6.00 +/- 0.00\n",
      "Step 15000\t平均奖励: -3.50 +/- 0.00\n",
      "Step 20000\t平均奖励: -1.80 +/- 0.00\n",
      "Step 25000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 30000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 35000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 40000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 45000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 50000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 55000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 60000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 65000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 70000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 75000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 80000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 85000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 90000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 95000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 100000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 105000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 110000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 115000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 120000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 125000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 130000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 135000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 140000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 145000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 150000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 155000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 160000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 165000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 170000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 175000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 180000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 185000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 190000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 195000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 200000\t平均奖励: 4.00 +/- 0.00\n",
      "Step 205000\t平均奖励: 4.00 +/- 0.00\n",
      "Step 210000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 215000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 220000\t平均奖励: 1.50 +/- 0.00\n",
      "Step 225000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 230000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 235000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 240000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 245000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 250000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 255000\t平均奖励: 1.50 +/- 0.00\n",
      "Step 260000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 265000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 270000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 275000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 280000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 285000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 290000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 295000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 300000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 305000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 310000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 315000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 320000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 325000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 330000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 335000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 340000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 345000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 350000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 355000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 360000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 365000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 370000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 375000\t平均奖励: 0.00 +/- 0.00\n",
      "Step 380000\t平均奖励: 4.00 +/- 0.00\n",
      "Step 385000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 390000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 395000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 400000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 405000\t平均奖励: -0.40 +/- 0.00\n",
      "Step 410000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 415000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 420000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 425000\t平均奖励: 3.00 +/- 0.00\n",
      "Step 430000\t平均奖励: 4.00 +/- 0.00\n",
      "Step 435000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 440000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 445000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 450000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 455000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 460000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 465000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 470000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 475000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 480000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 485000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 490000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 495000\t平均奖励: 34.50 +/- 0.00\n",
      "Step 500000\t平均奖励: 34.50 +/- 0.00\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T09:18:45.304630Z",
     "start_time": "2024-12-30T09:18:44.697040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "'''\n",
    "agent.env = wrapped_eval_env\n",
    "print(\"Start evaluation...\")\n",
    "agent.evaluate(episodes=100)\n",
    "eval_env.close()\n",
    "'''\n",
    "print(\"Start evaluation...\")\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    wrapped_eval_env,\n",
    "    n_eval_episodes=100,\n",
    "    deterministic=True,  # 使用确定性策略进行评估\n",
    "    render=False\n",
    ")\n",
    "print(f\"Final Evaluation\\t平均奖励: {mean_reward:.2f} +/- {std_reward:.2f}\")\n"
   ],
   "id": "f5082eb8df7181c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluation...\n",
      "Final Evaluation\t平均奖励: 34.50 +/- 0.00\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T09:19:00.818650Z",
     "start_time": "2024-12-30T09:18:45.306637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_env = gym.make('FrozenLake-v1', is_slippery=False, render_mode = 'human', desc = map)#,map_name='4x4')#\n",
    "wrapped_final_env = FrozenLakeRewardWrapper(final_env)\n",
    "\n",
    "#agent.env = wrapped_final_env\n",
    "#model.set_env(wrapped_final_env)\n",
    "\n",
    "# 重置环境，获取初始状态\n",
    "state, info = wrapped_final_env.reset()\n",
    "\n",
    "done = False  # 游戏是否结束\n",
    "step_count = 0  # 计步\n",
    "\n",
    "while not done:\n",
    "    wrapped_final_env.render()  # 渲染环境\n",
    "    #action = agent.choose_best_action(state)  # 选择动作\n",
    "    action, _ = model.predict(state, deterministic=True)\n",
    "    state, reward, terminated, truncated, _ = wrapped_final_env.step(action[()])  # 执行动作\n",
    "    done = terminated or truncated\n",
    "    print(f\"Step {step_count}: Action={action}, State={state}, Reward={reward}, Done={done}\")\n",
    "    step_count += 1\n",
    "\n",
    "# 游戏结束后关闭环境\n",
    "wrapped_final_env.close()\n"
   ],
   "id": "d71e4ae8a9cafd2d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Action=1, State=16, Reward=0.5, Done=False\n",
      "Step 1: Action=1, State=32, Reward=0.5, Done=False\n",
      "Step 2: Action=2, State=33, Reward=0.5, Done=False\n",
      "Step 3: Action=1, State=49, Reward=0.5, Done=False\n",
      "Step 4: Action=2, State=50, Reward=0.5, Done=False\n",
      "Step 5: Action=2, State=51, Reward=0.5, Done=False\n",
      "Step 6: Action=2, State=52, Reward=0.5, Done=False\n",
      "Step 7: Action=2, State=53, Reward=0.5, Done=False\n",
      "Step 8: Action=2, State=54, Reward=0.5, Done=False\n",
      "Step 9: Action=1, State=70, Reward=0.5, Done=False\n",
      "Step 10: Action=1, State=86, Reward=0.5, Done=False\n",
      "Step 11: Action=1, State=102, Reward=0.5, Done=False\n",
      "Step 12: Action=1, State=118, Reward=0.5, Done=False\n",
      "Step 13: Action=1, State=134, Reward=0.5, Done=False\n",
      "Step 14: Action=2, State=135, Reward=0.5, Done=False\n",
      "Step 15: Action=1, State=151, Reward=0.5, Done=False\n",
      "Step 16: Action=2, State=152, Reward=0.5, Done=False\n",
      "Step 17: Action=2, State=153, Reward=0.5, Done=False\n",
      "Step 18: Action=1, State=169, Reward=0.5, Done=False\n",
      "Step 19: Action=2, State=170, Reward=0.5, Done=False\n",
      "Step 20: Action=2, State=171, Reward=0.5, Done=False\n",
      "Step 21: Action=2, State=172, Reward=0.5, Done=False\n",
      "Step 22: Action=2, State=173, Reward=0.5, Done=False\n",
      "Step 23: Action=1, State=189, Reward=0.5, Done=False\n",
      "Step 24: Action=2, State=190, Reward=0.5, Done=False\n",
      "Step 25: Action=1, State=206, Reward=0.5, Done=False\n",
      "Step 26: Action=2, State=207, Reward=0.5, Done=False\n",
      "Step 27: Action=1, State=223, Reward=0.5, Done=False\n",
      "Step 28: Action=1, State=239, Reward=0.5, Done=False\n",
      "Step 29: Action=1, State=255, Reward=20.0, Done=True\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
