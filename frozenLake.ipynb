{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-12T14:43:49.757322Z",
     "start_time": "2024-12-12T14:43:46.604602Z"
    }
   },
   "source": [
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import stable_baselines3\n",
    "import gymnasium as gym\n",
    "\n",
    "from FrozenLakeRewardWrapper import FrozenLakeRewardWrapper\n",
    "from PPOAgent import PPOAgent\n",
    "from QLearningAgent import QLearningAgent\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 创建 FrozenLake 环境 (Q-Learning)\n",
    "#map = generate_random_map(size=8)\n",
    "training_env1 = gym.make('FrozenLake-v1', map_name='4x4', is_slippery=True)#desc=map, is_slippery = True)#\n",
    "wrapped_training_env = FrozenLakeRewardWrapper(training_env1)\n",
    "\n",
    "eval_env = gym.make('FrozenLake-v1',map_name='4x4', is_slippery=True)# desc=map, is_slippery = True)#\n",
    "wrapped_eval_env = FrozenLakeRewardWrapper(eval_env)\n",
    "\n",
    "agent = QLearningAgent(wrapped_training_env)\n",
    "# agent = PPOAgent(wrapped_training_env1)\n",
    "\n",
    "print(\"Start training...\")\n",
    "agent.train(episodes=10000)\n",
    "wrapped_training_env.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Episode 100/10000 - Total Reward: -1.2, Epsilon: 0.6058\n",
      "Episode 200/10000 - Total Reward: -1.45, Epsilon: 0.3670\n",
      "Episode 300/10000 - Total Reward: -1.6, Epsilon: 0.2223\n",
      "Episode 400/10000 - Total Reward: 9.7, Epsilon: 0.1347\n",
      "Episode 500/10000 - Total Reward: 9.5, Epsilon: 0.0816\n",
      "Episode 600/10000 - Total Reward: -3.3499999999999996, Epsilon: 0.0494\n",
      "Episode 700/10000 - Total Reward: -4.99999999999999, Epsilon: 0.0299\n",
      "Episode 800/10000 - Total Reward: 7.100000000000002, Epsilon: 0.0181\n",
      "Episode 900/10000 - Total Reward: 5.950000000000006, Epsilon: 0.0110\n",
      "Episode 1000/10000 - Total Reward: -5.049999999999994, Epsilon: 0.0100\n",
      "Episode 1100/10000 - Total Reward: 7.0500000000000025, Epsilon: 0.0100\n",
      "Episode 1200/10000 - Total Reward: -1.9000000000000004, Epsilon: 0.0100\n",
      "Episode 1300/10000 - Total Reward: -2.650000000000001, Epsilon: 0.0100\n",
      "Episode 1400/10000 - Total Reward: -2.1500000000000004, Epsilon: 0.0100\n",
      "Episode 1500/10000 - Total Reward: 9.5, Epsilon: 0.0100\n",
      "Episode 1600/10000 - Total Reward: 9.0, Epsilon: 0.0100\n",
      "Episode 1700/10000 - Total Reward: 9.15, Epsilon: 0.0100\n",
      "Episode 1800/10000 - Total Reward: 7.75, Epsilon: 0.0100\n",
      "Episode 1900/10000 - Total Reward: -3.799999999999998, Epsilon: 0.0100\n",
      "Episode 2000/10000 - Total Reward: 6.850000000000003, Epsilon: 0.0100\n",
      "Episode 2100/10000 - Total Reward: -2.2, Epsilon: 0.0100\n",
      "Episode 2200/10000 - Total Reward: 7.8999999999999995, Epsilon: 0.0100\n",
      "Episode 2300/10000 - Total Reward: 8.75, Epsilon: 0.0100\n",
      "Episode 2400/10000 - Total Reward: 8.049999999999999, Epsilon: 0.0100\n",
      "Episode 2500/10000 - Total Reward: 9.4, Epsilon: 0.0100\n",
      "Episode 2600/10000 - Total Reward: 8.7, Epsilon: 0.0100\n",
      "Episode 2700/10000 - Total Reward: 5.3500000000000085, Epsilon: 0.0100\n",
      "Episode 2800/10000 - Total Reward: 8.6, Epsilon: 0.0100\n",
      "Episode 2900/10000 - Total Reward: -1.8000000000000003, Epsilon: 0.0100\n",
      "Episode 3000/10000 - Total Reward: 7.550000000000001, Epsilon: 0.0100\n",
      "Episode 3100/10000 - Total Reward: 7.250000000000002, Epsilon: 0.0100\n",
      "Episode 3200/10000 - Total Reward: 9.35, Epsilon: 0.0100\n",
      "Episode 3300/10000 - Total Reward: -2.2, Epsilon: 0.0100\n",
      "Episode 3400/10000 - Total Reward: -1.5499999999999998, Epsilon: 0.0100\n",
      "Episode 3500/10000 - Total Reward: -4.99999999999999, Epsilon: 0.0100\n",
      "Episode 3600/10000 - Total Reward: -5.599999999999992, Epsilon: 0.0100\n",
      "Episode 3700/10000 - Total Reward: -1.9500000000000002, Epsilon: 0.0100\n",
      "Episode 3800/10000 - Total Reward: 9.049999999999999, Epsilon: 0.0100\n",
      "Episode 3900/10000 - Total Reward: 6.100000000000006, Epsilon: 0.0100\n",
      "Episode 4000/10000 - Total Reward: 9.049999999999999, Epsilon: 0.0100\n",
      "Episode 4100/10000 - Total Reward: -2.6000000000000005, Epsilon: 0.0100\n",
      "Episode 4200/10000 - Total Reward: 7.949999999999999, Epsilon: 0.0100\n",
      "Episode 4300/10000 - Total Reward: 5.800000000000007, Epsilon: 0.0100\n",
      "Episode 4400/10000 - Total Reward: -1.9500000000000002, Epsilon: 0.0100\n",
      "Episode 4500/10000 - Total Reward: -5.099999999999993, Epsilon: 0.0100\n",
      "Episode 4600/10000 - Total Reward: -4.99999999999999, Epsilon: 0.0100\n",
      "Episode 4700/10000 - Total Reward: -4.049999999999997, Epsilon: 0.0100\n",
      "Episode 4800/10000 - Total Reward: 8.35, Epsilon: 0.0100\n",
      "Episode 4900/10000 - Total Reward: 9.4, Epsilon: 0.0100\n",
      "Episode 5000/10000 - Total Reward: 7.8999999999999995, Epsilon: 0.0100\n",
      "Episode 5100/10000 - Total Reward: -2.6000000000000005, Epsilon: 0.0100\n",
      "Episode 5200/10000 - Total Reward: 8.85, Epsilon: 0.0100\n",
      "Episode 5300/10000 - Total Reward: -4.99999999999999, Epsilon: 0.0100\n",
      "Episode 5400/10000 - Total Reward: 6.100000000000006, Epsilon: 0.0100\n",
      "Episode 5500/10000 - Total Reward: -1.5, Epsilon: 0.0100\n",
      "Episode 5600/10000 - Total Reward: 8.649999999999999, Epsilon: 0.0100\n",
      "Episode 5700/10000 - Total Reward: -2.3000000000000007, Epsilon: 0.0100\n",
      "Episode 5800/10000 - Total Reward: 7.550000000000001, Epsilon: 0.0100\n",
      "Episode 5900/10000 - Total Reward: -4.99999999999999, Epsilon: 0.0100\n",
      "Episode 6000/10000 - Total Reward: 6.900000000000003, Epsilon: 0.0100\n",
      "Episode 6100/10000 - Total Reward: 8.45, Epsilon: 0.0100\n",
      "Episode 6200/10000 - Total Reward: -2.1500000000000004, Epsilon: 0.0100\n",
      "Episode 6300/10000 - Total Reward: 7.85, Epsilon: 0.0100\n",
      "Episode 6400/10000 - Total Reward: 7.8, Epsilon: 0.0100\n",
      "Episode 6500/10000 - Total Reward: 7.6000000000000005, Epsilon: 0.0100\n",
      "Episode 6600/10000 - Total Reward: 9.1, Epsilon: 0.0100\n",
      "Episode 6700/10000 - Total Reward: 8.299999999999999, Epsilon: 0.0100\n",
      "Episode 6800/10000 - Total Reward: -1.6, Epsilon: 0.0100\n",
      "Episode 6900/10000 - Total Reward: 7.8999999999999995, Epsilon: 0.0100\n",
      "Episode 7000/10000 - Total Reward: -3.3, Epsilon: 0.0100\n",
      "Episode 7100/10000 - Total Reward: 8.45, Epsilon: 0.0100\n",
      "Episode 7200/10000 - Total Reward: 8.6, Epsilon: 0.0100\n",
      "Episode 7300/10000 - Total Reward: 7.65, Epsilon: 0.0100\n",
      "Episode 7400/10000 - Total Reward: 9.65, Epsilon: 0.0100\n",
      "Episode 7500/10000 - Total Reward: -3.3999999999999995, Epsilon: 0.0100\n",
      "Episode 7600/10000 - Total Reward: 9.0, Epsilon: 0.0100\n",
      "Episode 7700/10000 - Total Reward: 7.000000000000003, Epsilon: 0.0100\n",
      "Episode 7800/10000 - Total Reward: -3.25, Epsilon: 0.0100\n",
      "Episode 7900/10000 - Total Reward: -2.4500000000000006, Epsilon: 0.0100\n",
      "Episode 8000/10000 - Total Reward: -2.700000000000001, Epsilon: 0.0100\n",
      "Episode 8100/10000 - Total Reward: -1.3, Epsilon: 0.0100\n",
      "Episode 8200/10000 - Total Reward: -2.750000000000001, Epsilon: 0.0100\n",
      "Episode 8300/10000 - Total Reward: 9.15, Epsilon: 0.0100\n",
      "Episode 8400/10000 - Total Reward: -4.299999999999996, Epsilon: 0.0100\n",
      "Episode 8500/10000 - Total Reward: -2.2, Epsilon: 0.0100\n",
      "Episode 8600/10000 - Total Reward: -2.950000000000001, Epsilon: 0.0100\n",
      "Episode 8700/10000 - Total Reward: 9.55, Epsilon: 0.0100\n",
      "Episode 8800/10000 - Total Reward: -2.8000000000000007, Epsilon: 0.0100\n",
      "Episode 8900/10000 - Total Reward: 8.549999999999999, Epsilon: 0.0100\n",
      "Episode 9000/10000 - Total Reward: 9.049999999999999, Epsilon: 0.0100\n",
      "Episode 9100/10000 - Total Reward: 8.7, Epsilon: 0.0100\n",
      "Episode 9200/10000 - Total Reward: -1.65, Epsilon: 0.0100\n",
      "Episode 9300/10000 - Total Reward: -3.0500000000000007, Epsilon: 0.0100\n",
      "Episode 9400/10000 - Total Reward: 9.3, Epsilon: 0.0100\n",
      "Episode 9500/10000 - Total Reward: -2.4000000000000004, Epsilon: 0.0100\n",
      "Episode 9600/10000 - Total Reward: -4.549999999999995, Epsilon: 0.0100\n",
      "Episode 9700/10000 - Total Reward: -4.549999999999995, Epsilon: 0.0100\n",
      "Episode 9800/10000 - Total Reward: 9.35, Epsilon: 0.0100\n",
      "Episode 9900/10000 - Total Reward: 9.5, Epsilon: 0.0100\n",
      "Episode 10000/10000 - Total Reward: 7.75, Epsilon: 0.0100\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:44:00.617600Z",
     "start_time": "2024-12-12T14:44:00.588876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent.env = wrapped_eval_env\n",
    "print(\"Start evaluation...\")\n",
    "agent.evaluate(episodes=100)\n",
    "eval_env.close()"
   ],
   "id": "f5082eb8df7181c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluation...\n",
      "Success Rate: 0.79\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:47:46.646787Z",
     "start_time": "2024-12-12T14:47:32.749260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_env = gym.make('FrozenLake-v1', is_slippery=True, render_mode = 'human',map_name='4x4')# desc = map)#,\n",
    "wrapped_final_env = FrozenLakeRewardWrapper(final_env)\n",
    "\n",
    "#agent.env = wrapped_final_env\n",
    "#model.set_env(wrapped_final_env)\n",
    "\n",
    "# 重置环境，获取初始状态\n",
    "state, info = wrapped_final_env.reset()\n",
    "\n",
    "done = False  # 游戏是否结束\n",
    "step_count = 0  # 计步\n",
    "\n",
    "while not done:\n",
    "    wrapped_final_env.render()  # 渲染环境\n",
    "    action = agent.choose_best_action(state)  # 选择动作\n",
    "    state, reward, terminated, truncated, _ = wrapped_final_env.step(action)  # 执行动作\n",
    "    done = terminated or truncated\n",
    "    print(f\"Step {step_count}: Action={action}, State={state}, Reward={reward}, Done={done}\")\n",
    "    step_count += 1\n",
    "\n",
    "# 游戏结束后关闭环境\n",
    "wrapped_final_env.close()\n"
   ],
   "id": "d71e4ae8a9cafd2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Action=0, State=0, Reward=-0.05, Done=False\n",
      "Step 1: Action=0, State=0, Reward=-0.05, Done=False\n",
      "Step 2: Action=0, State=4, Reward=-0.05, Done=False\n",
      "Step 3: Action=0, State=0, Reward=-0.05, Done=False\n",
      "Step 4: Action=0, State=4, Reward=-0.05, Done=False\n",
      "Step 5: Action=0, State=4, Reward=-0.05, Done=False\n",
      "Step 6: Action=0, State=0, Reward=-0.05, Done=False\n",
      "Step 7: Action=0, State=4, Reward=-0.05, Done=False\n",
      "Step 8: Action=0, State=8, Reward=-0.05, Done=False\n",
      "Step 9: Action=3, State=8, Reward=-0.05, Done=False\n",
      "Step 10: Action=3, State=8, Reward=-0.05, Done=False\n",
      "Step 11: Action=3, State=9, Reward=-0.05, Done=False\n",
      "Step 12: Action=1, State=8, Reward=-0.05, Done=False\n",
      "Step 13: Action=3, State=4, Reward=-0.05, Done=False\n",
      "Step 14: Action=0, State=8, Reward=-0.05, Done=False\n",
      "Step 15: Action=3, State=9, Reward=-0.05, Done=False\n",
      "Step 16: Action=1, State=13, Reward=-0.05, Done=False\n",
      "Step 17: Action=2, State=9, Reward=-0.05, Done=False\n",
      "Step 18: Action=1, State=8, Reward=-0.05, Done=False\n",
      "Step 19: Action=3, State=8, Reward=-0.05, Done=False\n",
      "Step 20: Action=3, State=4, Reward=-0.05, Done=False\n",
      "Step 21: Action=0, State=8, Reward=-0.05, Done=False\n",
      "Step 22: Action=3, State=9, Reward=-0.05, Done=False\n",
      "Step 23: Action=1, State=13, Reward=-0.05, Done=False\n",
      "Step 24: Action=2, State=14, Reward=-0.05, Done=False\n",
      "Step 25: Action=1, State=14, Reward=-0.05, Done=False\n",
      "Step 26: Action=1, State=15, Reward=10.0, Done=True\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
